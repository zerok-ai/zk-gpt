from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate

from app.models.llm_output_parser import PrometheusLLMOutputParser, PrometheusDataSummary


class PromptFactory:
    # issue_summary = """You are a backend developer AI assistant. Your task is to figure out why an issue \
    # happened and present it in a concise manner wrt exception,latency,out of memory, service unreacable etc. The issue here is defined as {issue_prompt}.
    # We have collected {issue_data} and will feed them to you one by one. Come up with the \
    # likeliest cause based on the data presented to you. 
    # """

    exception_data = """You are a backend developer AI assistant.
    Your task is to figure out why an issue happened and present it in a concise manner.
    The issue's summary is here defined as {issue_data}. We have collected exceptions stack trace occured across spans as {exception_data}.
    Come up with the likeliest cause based on the exception data presented to you and brief the issue's exception stack trace if so. 
    """

    trace_data = """You are a backend developer AI assistant.
    Your task is to figure out why an issue happened and present it in a concise manner.
    The issue here is defined as {exception_summary}. We have collected spans data of the trace of the given issue  \
    and will feed them to you one by one as {trace_data}.
    Come up with the likeliest cause based on the spans data presented to you. and brief the sources and destinations and spans that are likely causing the issue.
    """

    request_response_payload = """You are a backend developer AI assistant.
    Your task is to figure out why an issue happened and present it in a concise manner.
    The issue's summary is here defined as {trace_summary}. We have collected request and response payload across all the spans of the trace of the given issue as {req_res_data}.
    Come up with the likeliest cause based on the request, response data presented to you brief the issue if there are any anolomy in the data. 
    """

    final_summary_prompt = """As a backend developer AI assistant, your primary objective is to identify the root cause of an issue,
    summarize it, and provide a concise explanation. The issue's
    exception stack trace is summarized as {exception_summary}. Additionally,{trace_summary} \
    is a summary of the trace data collected for this issue, and {req_res_summary} \
    is the summary of the request-response payload pertaining to this issue,Your task is to determine \
    the most likely cause based on the provided summary data in 3 or 4 lines without leaving any key data points. in a saperate line highlight any anomalous data points if present as "anomalies".
    """

    fetch_prom_queries_prompt = """
        Generate PromQL queries relevant to investigate and monitor an issue in the service based on the given data. \
        that includes the issue title : {issue_title}, an inference generated by GPT using exception stacktrace data, \
        request-response payload, trace_data as {issue_inference}. Utilize the provided data JSON : {data} to construct PromQL queries as python dict where key is query title and valus is promQL query wrt prometheus service \
        that can help verify the issue's impact on the service's metrics.
        """

    fetch_prom_queries_prompt_v2 = """
        You are an on-call engineer. Generate relevant PromQL queries to investigate an issue in the service based on the given data. \
        The issue is defined as : {issue_title}, a likely cause inference generated by GPT using exception stacktrace data, \
        request-response payload, trace_data as {issue_inference}. Utilize the provided data JSON : {data} to construct PromQL queries \
        for relevant metrics that you as an on-call engineer would look at to root-cause the issue. The response must have only the list of PromQL queries as array of string and should not contain line breaks or special characters.. No english sentences must be present.
        Utilize the Prometheus Helm, version v0.68.0 from prometheus-community/kube-prometheus-stack and also queries starting http_ should have prefix kubelet_
        """

    promql_query_generation_from_definition = """An alert has been triggered in my service with alert definition : \
    {alert_definition}. To investigate the issue thoroughly, I need assistance in generating specific 10 PromQL \
    queries on kubernetes metrics server.These queries will enable me to analyze the system based on the provided \
    alert. Generate response with formatted instruction as {format_instructions}. do not add any \
    other elements to the response."""

    prompt_for_prom_metric_data_summary = """When performing following query on prometheus: {query} with title \
    description : {title} Received following time series data: {query_metric_data}.Summarise \
    anomalies or issues present in the provided data without leaving any key data points \
    .Generate response with formatted instruction as {format_instructions}. do not add any other \
    elements to the response."""

    pod_k8s_events = """<explain about before summary>
    {input} <brief about the current data> {custom_data}"""

    deploy_k8s_events = """<explain about before summary>
    {input} <brief about the current data> {custom_data}"""

    configmap_k8s_events = """<explain about before summary>
    {input} <brief about the current data> {custom_data}"""

    cpu_usage_events = """<explain about before summary>
    {input} <brief about the current data> {custom_data}"""

    log_data = """<explain about before summary>
    {input} <brief about the current data> {custom_data}"""

    memory_usage_events = """<explain about before summary>
    {input} <brief about the current data> {custom_data}"""

    # user_query_prompt = """ your key task is \
    # to respond to the user's query, "{query}," by drawing upon both the system user context provided by \
    # "{user_qna_context_data}" and the given issue summarized as {issue_summary} and relevant Pinecone similarity  \
    # documents retrieved for the specific query, presented as "Documents: {pinecone_similarity_docs}.
    # """

    user_query_prompt = """ your key task is \
        to respond to the user's query, "{query}, the context for this query will be within exception data : {exception_data}, trace data as : \
         {trace_data} and request response payloads across spans as : {request_response_payload} \
        """

    prompt_infos = [
        {
            'name': 'exception_data',
            'description': 'Template used to inference the issue using exception data',
            'prompt_template': exception_data,
            "input_variables": ["issue_data", "exception_data"],
            "output_variables": "exception_summary"
        },
        {
            'name': 'trace_data',
            'description': 'Template used to inference the issue using trace data',
            'prompt_template': trace_data,
            "input_variables": ["exception_summary", "trace_data"],
            "output_variables": "trace_summary"
        },
        {
            'name': 'request_response_payload',
            'description': 'Template used to inference the issue using request response data',
            'prompt_template': request_response_payload,
            "input_variables": ["trace_summary", "req_res_data"],
            "output_variables": "req_res_summary"
        },
        {
            'name': 'final summary',
            'description': 'Template used to inference the issue with final summary',
            'prompt_template': final_summary_prompt,
            "input_variables": ["exception_summary", "trace_summary", "req_res_summary"],
            "output_variables": "final_summary"
        }
    ]

    user_query_prompt_infos = [
        {
            'name': 'User Query Prompt',
            'description': 'Template used to respond to the users query',
            'prompt_template': user_query_prompt,
            "input_variables": ["query", "trace_data", "exception_data", "request_response_payload"],
            "output_variables": "user_query_response"
        }
    ]

    promql_prompt_infos = [
        {
            'name': 'fetch_prom_queries',
            'description': 'Template used to fetch prometheus queries using incident inference',
            'prompt_template': fetch_prom_queries_prompt_v2,
            "input_variables": ["issue_title", "issue_inference", "data"],
            "output_variables": "promql_queries"
        }
    ]

    prompt_template_for_promql_queries = [
        {
            'name': 'prompt_template_for_promql_queries',
            'description': 'Template used to fetch prometheus queries using alert definition',
            'prompt_template': promql_query_generation_from_definition,
            "input_variables": ["alert_definition"],
            "output_variables": "promql_queries"
        }
    ]

    prompt_template_for_prom_data_summary = [
        {
            'name': 'prompt_template_for_prom_data_summary',
            'description': 'Template used to summarise prometheus queries metric data',
            'prompt_template': prompt_for_prom_metric_data_summary,
            "input_variables": ["query", "title", "query_metric_data"],
            "output_variables": "prom_summary"
        }
    ]

    # langchain_agent_prompting = [
    #     {
    #         'name': 'prompt_template_for_langchain_agents',
    #         'description': 'Template used to create agents and use different tools for a given langchain agent',
    #         'prompt_template': prompt_for_prom_metric_data_summary,
    #         "input_variables": ["alert_definition"],
    #         "output_variables": "promql_queries"
    #     }
    # ]

    def get_all_prompts(self):
        return self.prompt_infos

    def get_all_user_query_prompts(self):
        return self.user_query_prompt_infos

    def get_all_promql_query_prompts(self):
        return self.promql_prompt_infos

    def get_prompt_template_for_promql_queries(self):
        return self.prompt_template_for_promql_queries

    def get_prom_template_for_summary_from_metric_data(self):
        return self.prompt_template_for_prom_data_summary

    def generate_prompts_for_sequential_chain(self):
        prompts = []
        output_keys = []

        prompt_templates = self.get_all_prompts()
        for prompt_tem in prompt_templates:
            prompt_template = PromptTemplate(input_variables=prompt_tem['input_variables'],
                                             template=prompt_tem['prompt_template'])
            prompts.append(prompt_template)
            output_keys.append(prompt_tem['output_variables'])
        return prompts, output_keys

    def generate_prompts_for_user_query_sequential_chain(self):
        prompts = []
        output_keys = []

        prompt_templates = self.get_all_user_query_prompts()
        for prompt_tem in prompt_templates:
            prompt_template = PromptTemplate(input_variables=prompt_tem['input_variables'],
                                             template=prompt_tem['prompt_template'])
            prompts.append(prompt_template)
            output_keys.append(prompt_tem['output_variables'])
        return prompts, output_keys

    def generate_prompts_for_promql_queries_sequential_chain(self):
        prompts = []
        output_keys = []

        prompt_templates = self.get_all_promql_query_prompts()
        for prompt_tem in prompt_templates:
            prompt_template = PromptTemplate(input_variables=prompt_tem['input_variables'],
                                             template=prompt_tem['prompt_template'])
            prompts.append(prompt_template)
            output_keys.append(prompt_tem['output_variables'])
        return prompts, output_keys

    def prompt_template_for_promql_queries_sequential_chain(self):
        prompts = []
        output_keys = []
        parser = PydanticOutputParser(pydantic_object=PrometheusLLMOutputParser)
        prompt_templates = self.get_prompt_template_for_promql_queries()
        for prompt_tem in prompt_templates:
            prompt_template = PromptTemplate(input_variables=prompt_tem['input_variables'],
                                             template=prompt_tem['prompt_template'],
                                             partial_variables={
                                                 "format_instructions": parser.get_format_instructions()},
                                             )
            prompts.append(prompt_template)
            output_keys.append(prompt_tem['output_variables'])
        return prompts, output_keys

    def prompt_template_for_prom_summary_from_metric_data(self):
        prompts = []
        output_keys = []
        parser = PydanticOutputParser(pydantic_object=PrometheusDataSummary)
        prompt_templates = self.get_prom_template_for_summary_from_metric_data()
        for prompt_tem in prompt_templates:
            prompt_template = PromptTemplate(input_variables=prompt_tem['input_variables'],
                                             template=prompt_tem['prompt_template'],
                                             partial_variables={
                                                 "format_instructions": parser.get_format_instructions()},
                                             )
            prompts.append(prompt_template)
            output_keys.append(prompt_tem['output_variables'])
        return prompts, output_keys
